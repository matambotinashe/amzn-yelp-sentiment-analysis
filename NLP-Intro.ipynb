{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP-Intro.ipynb","provenance":[],"authorship_tag":"ABX9TyNCm3/A9knKDC/Bal/GS9cz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **NLP Introduction**\n","- In this notebook we are introduced to Natural Language Processing.\n","- We be looking at concerts that help us conver text to data that can be used in ML and DL model.\n","\n","## **Libraries**\n","- We will making user of tensoflow in this notebook specifical the preprocessing text and sequence"],"metadata":{"id":"JQMHpKyxD_Ml"}},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"WhHAAJP70xrD","executionInfo":{"status":"ok","timestamp":1657351214717,"user_tz":-120,"elapsed":5680,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["Tokenizer = tf.keras.preprocessing.text.Tokenizer\n","pad_sequences = tf.keras.preprocessing.sequence.pad_sequences1"],"metadata":{"id":"uUFWQuCO0uar","executionInfo":{"status":"ok","timestamp":1657351214718,"user_tz":-120,"elapsed":20,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## **Data**\n","- For demostration we create some sentences we will be using as data. \n","- *Sentence created for some of saying and song to spang in mind*üòè"],"metadata":{"id":"KrjFIHr_FJpW"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHKc9BL1DbXj","executionInfo":{"status":"ok","timestamp":1657351214718,"user_tz":-120,"elapsed":20,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"765b1f8c-882d-4c83-8bd4-a2dd5ccbb359"},"outputs":[{"output_type":"stream","name":"stdout","text":["['My very earnest mother', 'Just sent us', 'No porridge please', 'Yummy yummy make my tummy bummy', 'Up up away it goes']\n"]}],"source":["sentences = [\n","             'My very earnest mother',\n","             'Just sent us',\n","             'No porridge please',\n","             'Yummy yummy make my tummy bummy',\n","             'Up up away it goes',\n","             'Mcdonald had a farm',\n","             'The farm had some goats',\n","             'The goats had some kids',\n","             'A meeh here a meeh there',\n","             'Everywhere meeh meeh',\n","             'Beast of England, beast of Ireland',\n","             'Beast of every land and clime',\n","             'Hence forth to my joyful tiding',\n","             'Of the future golden times',\n","             'Soon or late day shall come',\n","             'When man shall be over throne'\n","]\n","print(sentences[:5])"]},{"cell_type":"markdown","source":["## **Tokenizer**\n","- Now will create our word tokenizer.\n","- We will need to specify the maximum number of words in our dictionary and in this case will use 100.\n","- This number represents the most common words in our sentences.\n","- We will also be using out of vocabulary(OOV),for cases in which we get sentence that have some words not contained in our dictionary. \n"],"metadata":{"id":"o9DCOHfwIFG6"}},{"cell_type":"code","source":["tokenizer = Tokenizer(num_words= 100, oov_token= '<OOV>')"],"metadata":{"id":"lkt1tG79H0WI","executionInfo":{"status":"ok","timestamp":1657351214718,"user_tz":-120,"elapsed":16,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["__Comments__\n","- Now will applie this to our sentences.\n","- From this we will be able to generate word index which will have number representing our words."],"metadata":{"id":"BIRMkaC0I9Vn"}},{"cell_type":"code","source":["tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r54cZu1rI7ZU","executionInfo":{"status":"ok","timestamp":1657351214719,"user_tz":-120,"elapsed":17,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"c2ab4f5f-8483-4b66-8dbd-7bc9e2640f7a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'meeh': 2, 'of': 3, 'my': 4, 'had': 5, 'a': 6, 'the': 7, 'beast': 8, 'yummy': 9, 'up': 10, 'farm': 11, 'some': 12, 'goats': 13, 'shall': 14, 'very': 15, 'earnest': 16, 'mother': 17, 'just': 18, 'sent': 19, 'us': 20, 'no': 21, 'porridge': 22, 'please': 23, 'make': 24, 'tummy': 25, 'bummy': 26, 'away': 27, 'it': 28, 'goes': 29, 'mcdonald': 30, 'kids': 31, 'here': 32, 'there': 33, 'everywhere': 34, 'england': 35, 'ireland': 36, 'every': 37, 'land': 38, 'and': 39, 'clime': 40, 'hence': 41, 'forth': 42, 'to': 43, 'joyful': 44, 'tiding': 45, 'future': 46, 'golden': 47, 'times': 48, 'soon': 49, 'or': 50, 'late': 51, 'day': 52, 'come': 53, 'when': 54, 'man': 55, 'be': 56, 'over': 57, 'throne': 58}\n"]}]},{"cell_type":"markdown","source":["## **Sentences to Sequences**\n","- Now will convert our sequences to sequence of number from the word index we have just created."],"metadata":{"id":"E_WlcJrqJfUm"}},{"cell_type":"code","source":["sequences = tokenizer.texts_to_sequences(sentences)\n","print(sequences[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03W4rss_Jv7Q","executionInfo":{"status":"ok","timestamp":1657351214719,"user_tz":-120,"elapsed":16,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"c6c7ca2a-b2a3-4564-f497-e07c184d63fd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4, 15, 16, 17], [18, 19, 20], [21, 22, 23], [9, 9, 24, 4, 25, 26], [10, 10, 27, 28, 29]]\n"]}]},{"cell_type":"markdown","source":["## **Padding Sequences**\n","- For our modeling process we need to make sure that the inputs are of the same length.\n","- The sentences we have created are of different length, resulting in sequence of different sizes.\n","- To command this will make use of padding, we add zeros to short sequences either and the beginning or at the end of the sequences.\n","- We can also determine the maximum number of inputs we want in a sequence hence will cut sequences long that the maximum number of inputs specified."],"metadata":{"id":"mTAysyZtKLlz"}},{"cell_type":"code","source":["padded_sequences = pad_sequences(sequences)\n","print('Original sequences = ', sequences)\n","print('Padded sequences = ', padded_sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCd49dvPLAAY","executionInfo":{"status":"ok","timestamp":1657351214719,"user_tz":-120,"elapsed":14,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"81edc96c-a671-44be-b828-081f2aca4e4a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Original sequences =  [[4, 15, 16, 17], [18, 19, 20], [21, 22, 23], [9, 9, 24, 4, 25, 26], [10, 10, 27, 28, 29], [30, 5, 6, 11], [7, 11, 5, 12, 13], [7, 13, 5, 12, 31], [6, 2, 32, 6, 2, 33], [34, 2, 2], [8, 3, 35, 8, 3, 36], [8, 3, 37, 38, 39, 40], [41, 42, 43, 4, 44, 45], [3, 7, 46, 47, 48], [49, 50, 51, 52, 14, 53], [54, 55, 14, 56, 57, 58]]\n","Padded sequences =  [[ 0  0  4 15 16 17]\n"," [ 0  0  0 18 19 20]\n"," [ 0  0  0 21 22 23]\n"," [ 9  9 24  4 25 26]\n"," [ 0 10 10 27 28 29]\n"," [ 0  0 30  5  6 11]\n"," [ 0  7 11  5 12 13]\n"," [ 0  7 13  5 12 31]\n"," [ 6  2 32  6  2 33]\n"," [ 0  0  0 34  2  2]\n"," [ 8  3 35  8  3 36]\n"," [ 8  3 37 38 39 40]\n"," [41 42 43  4 44 45]\n"," [ 0  3  7 46 47 48]\n"," [49 50 51 52 14 53]\n"," [54 55 14 56 57 58]]\n"]}]},{"cell_type":"markdown","source":["__Comments__\n","- We will specify the maximum lenght we want in our sequence."],"metadata":{"id":"anRCjebrL4IX"}},{"cell_type":"code","source":["padded_sequences1 = pad_sequences(sequences, maxlen = 5)\n","print('Original sequences = ', sequences)\n","print('Padded sequences = ', padded_sequences1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpRTXZh5LvZt","executionInfo":{"status":"ok","timestamp":1657351214720,"user_tz":-120,"elapsed":14,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"452785f3-dd40-4bd2-ff19-0e30faa70118"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Original sequences =  [[4, 15, 16, 17], [18, 19, 20], [21, 22, 23], [9, 9, 24, 4, 25, 26], [10, 10, 27, 28, 29], [30, 5, 6, 11], [7, 11, 5, 12, 13], [7, 13, 5, 12, 31], [6, 2, 32, 6, 2, 33], [34, 2, 2], [8, 3, 35, 8, 3, 36], [8, 3, 37, 38, 39, 40], [41, 42, 43, 4, 44, 45], [3, 7, 46, 47, 48], [49, 50, 51, 52, 14, 53], [54, 55, 14, 56, 57, 58]]\n","Padded sequences =  [[ 0  4 15 16 17]\n"," [ 0  0 18 19 20]\n"," [ 0  0 21 22 23]\n"," [ 9 24  4 25 26]\n"," [10 10 27 28 29]\n"," [ 0 30  5  6 11]\n"," [ 7 11  5 12 13]\n"," [ 7 13  5 12 31]\n"," [ 2 32  6  2 33]\n"," [ 0  0 34  2  2]\n"," [ 3 35  8  3 36]\n"," [ 3 37 38 39 40]\n"," [42 43  4 44 45]\n"," [ 3  7 46 47 48]\n"," [50 51 52 14 53]\n"," [55 14 56 57 58]]\n"]}]},{"cell_type":"markdown","source":["__Comments__\n","- Instead of the default paddig at the beginning of the sequence will now apply padding right at the end of the sequence.\n","- We will still maintain our maximum length of 5."],"metadata":{"id":"9h7jhFK4MQ7_"}},{"cell_type":"code","source":["padded_sequences2 = pad_sequences(sequences, maxlen = 5, padding = 'post')\n","print('Original sequences = ', sequences)\n","print('Padded sequences = ', padded_sequences2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWdTKHfXMKMg","executionInfo":{"status":"ok","timestamp":1657351214720,"user_tz":-120,"elapsed":13,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"1f22a791-37b8-40a5-9e6b-a2bc0c1b1dd1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original sequences =  [[4, 15, 16, 17], [18, 19, 20], [21, 22, 23], [9, 9, 24, 4, 25, 26], [10, 10, 27, 28, 29], [30, 5, 6, 11], [7, 11, 5, 12, 13], [7, 13, 5, 12, 31], [6, 2, 32, 6, 2, 33], [34, 2, 2], [8, 3, 35, 8, 3, 36], [8, 3, 37, 38, 39, 40], [41, 42, 43, 4, 44, 45], [3, 7, 46, 47, 48], [49, 50, 51, 52, 14, 53], [54, 55, 14, 56, 57, 58]]\n","Padded sequences =  [[ 4 15 16 17  0]\n"," [18 19 20  0  0]\n"," [21 22 23  0  0]\n"," [ 9 24  4 25 26]\n"," [10 10 27 28 29]\n"," [30  5  6 11  0]\n"," [ 7 11  5 12 13]\n"," [ 7 13  5 12 31]\n"," [ 2 32  6  2 33]\n"," [34  2  2  0  0]\n"," [ 3 35  8  3 36]\n"," [ 3 37 38 39 40]\n"," [42 43  4 44 45]\n"," [ 3  7 46 47 48]\n"," [50 51 52 14 53]\n"," [55 14 56 57 58]]\n"]}]},{"cell_type":"markdown","source":["__Comments__\n","- Now will change the maximum length of the sequence to 15, which is longer than may sentence we have."],"metadata":{"id":"cPvtGYYwMs39"}},{"cell_type":"code","source":["padded_sequences3 = pad_sequences(sequences, maxlen = 15, padding = 'post')\n","print('Original sequences = ', sequences)\n","print('Padded sequences = ', padded_sequences3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7rpPozeMlaF","executionInfo":{"status":"ok","timestamp":1657351214721,"user_tz":-120,"elapsed":13,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"4c5ca240-5110-4f38-fed1-2ba9e584d67f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Original sequences =  [[4, 15, 16, 17], [18, 19, 20], [21, 22, 23], [9, 9, 24, 4, 25, 26], [10, 10, 27, 28, 29], [30, 5, 6, 11], [7, 11, 5, 12, 13], [7, 13, 5, 12, 31], [6, 2, 32, 6, 2, 33], [34, 2, 2], [8, 3, 35, 8, 3, 36], [8, 3, 37, 38, 39, 40], [41, 42, 43, 4, 44, 45], [3, 7, 46, 47, 48], [49, 50, 51, 52, 14, 53], [54, 55, 14, 56, 57, 58]]\n","Padded sequences =  [[ 4 15 16 17  0  0  0  0  0  0  0  0  0  0  0]\n"," [18 19 20  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [21 22 23  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 9  9 24  4 25 26  0  0  0  0  0  0  0  0  0]\n"," [10 10 27 28 29  0  0  0  0  0  0  0  0  0  0]\n"," [30  5  6 11  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 7 11  5 12 13  0  0  0  0  0  0  0  0  0  0]\n"," [ 7 13  5 12 31  0  0  0  0  0  0  0  0  0  0]\n"," [ 6  2 32  6  2 33  0  0  0  0  0  0  0  0  0]\n"," [34  2  2  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 8  3 35  8  3 36  0  0  0  0  0  0  0  0  0]\n"," [ 8  3 37 38 39 40  0  0  0  0  0  0  0  0  0]\n"," [41 42 43  4 44 45  0  0  0  0  0  0  0  0  0]\n"," [ 3  7 46 47 48  0  0  0  0  0  0  0  0  0  0]\n"," [49 50 51 52 14 53  0  0  0  0  0  0  0  0  0]\n"," [54 55 14 56 57 58  0  0  0  0  0  0  0  0  0]]\n"]}]},{"cell_type":"markdown","source":["## **OOV Application**\n","- Now will demostrate what happens when you have sentence with words outside our dictionary\n","- Will generate some new sentences ( *inspired by the empty youghut drink on my desk hence will call it yog* üòÜ)"],"metadata":{"id":"upsYZSe-NFJB"}},{"cell_type":"code","source":["yog_sentences = [\n","                'Chocolate milk i hate both',\n","                'I still take hot coco',\n","                'Forest berry yummy yummy',\n","                'Caramel is the best with lemons',\n","                'The low fat berry got me pumped'\n","]\n","print(yog_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCgKjwgfM-2L","executionInfo":{"status":"ok","timestamp":1657351214721,"user_tz":-120,"elapsed":12,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"74c012fe-47c4-4482-db2e-dd51d6b21a29"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['Chocolate milk i hate both', 'I still take hot coco', 'Forest berry yummy yummy', 'Caramel is the best with lemons', 'The low fat berry got me pumped']\n"]}]},{"cell_type":"markdown","source":["__Comments__\n","- Now will generate sequence for our new sentences."],"metadata":{"id":"puM8840bOfSP"}},{"cell_type":"code","source":["yog_sequence = tokenizer.texts_to_sequences(yog_sentences)\n","print('yog Sequence = ', yog_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3VSNQdzORAh","executionInfo":{"status":"ok","timestamp":1657351214721,"user_tz":-120,"elapsed":10,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"cc4d5292-8cb8-4c53-b283-5dd86e54332f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["yog Sequence =  [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 9, 9], [1, 1, 7, 1, 1, 1], [7, 1, 1, 1, 1, 1, 1]]\n"]}]},{"cell_type":"markdown","source":["__Comments__\n","- Every time we have a 1, it represents a word not in the previous generate word index.\n","- Final we demostrate applying max len and padding."],"metadata":{"id":"tqAXRMH3PGSm"}},{"cell_type":"code","source":["padded_yog_sequence = pad_sequences(yog_sequence, maxlen = 15, padding = 'post')\n","print('Original yog sequences = ', yog_sequence)\n","print('Padded yog sequences = ', padded_yog_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sC-qFZ3fO9Tf","executionInfo":{"status":"ok","timestamp":1657351214722,"user_tz":-120,"elapsed":10,"user":{"displayName":"tinashe matambo","userId":"08023646371386508054"}},"outputId":"c8f25495-dd34-4104-fee8-69ef09b546ff"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Original yog sequences =  [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 9, 9], [1, 1, 7, 1, 1, 1], [7, 1, 1, 1, 1, 1, 1]]\n","Padded yog sequences =  [[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"," [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"," [1 1 9 9 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 1 7 1 1 1 0 0 0 0 0 0 0 0 0]\n"," [7 1 1 1 1 1 1 0 0 0 0 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["## **Conclusion**\n","- Now that we have basing understanding of how we can convert test in data for ML and DL models, next will apply this on sentiment analysis."],"metadata":{"id":"3K_S3zX9yzDe"}}]}